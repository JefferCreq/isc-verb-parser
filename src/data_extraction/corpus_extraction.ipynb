{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9352,"status":"ok","timestamp":1725882980332,"user":{"displayName":"Jefferson Castro Reque","userId":"00769565088665449256"},"user_tz":300},"id":"u89_1y3aaPMP","outputId":"76444bb4-3a8b-46af-b85b-445790a0b087"},"outputs":[],"source":["# %pip install PyPDF2\n","# %pip install pdfplumber"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":697,"status":"ok","timestamp":1725882981028,"user":{"displayName":"Jefferson Castro Reque","userId":"00769565088665449256"},"user_tz":300},"id":"Ye9JWqM3aM-2"},"outputs":[],"source":["import re\n","import PyPDF2\n","import pandas as pd\n","import pdfplumber\n","from collections import defaultdict\n","# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Configurar opciones de visualización de pandas\n","pd.set_option('display.max_rows', None)\n","pd.set_option('display.max_columns', None)\n","pd.set_option('display.width', None)\n","pd.set_option('display.max_colwidth', None)"]},{"cell_type":"markdown","metadata":{"id":"eu4MYW5DQ4IF"},"source":["# Funciones"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":300,"status":"ok","timestamp":1725883035013,"user":{"displayName":"Jefferson Castro Reque","userId":"00769565088665449256"},"user_tz":300},"id":"FUkBacgHRViU"},"outputs":[],"source":["## Open PDF file\n","def open_pdf(file_path):\n","    with open(file_path, 'rb') as pdf_file:\n","        pdf_reader = PyPDF2.PdfReader(pdf_file)\n","        content = \"\"\n","        for page_num in range(len(pdf_reader.pages)):\n","            page = pdf_reader.pages[page_num]\n","            page_text = page.extract_text()\n","            if page_text:\n","                # Add custom logic here to add line breaks if necessary\n","                content += page_text + \"\\n\"\n","        return content"]},{"cell_type":"markdown","metadata":{"id":"uF1-0PLDQ64Q"},"source":["# Extraer Verbos del Vocabulario"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"executionInfo":{"elapsed":21580,"status":"ok","timestamp":1724140750355,"user":{"displayName":"Jefferson Castro Reque","userId":"00769565088665449256"},"user_tz":300},"id":"irH10ZSXrj0H","outputId":"f752f4de-76f5-433a-85e5-385bab7f1cae"},"outputs":[],"source":["\n","# Open the PDF file\n","with pdfplumber.open('../pdf/vocabulario_iskonawa.pdf') as pdf:\n","    content_plumber = \"\"\n","    for page in pdf.pages:\n","        # Get page dimensions\n","        width = page.width\n","        height = page.height\n","\n","        # Define the column boundaries (adjust these values based on page dimensions)\n","        left_column = (0, 0, width / 2, height)  # Example for left half of the page\n","        right_column = (width / 2, 0, width, height)  # Example for right half of the page\n","\n","        def extract_column_text(page, column_bounds):\n","            left, top, right, bottom = column_bounds\n","            if right <= width and bottom <= height:\n","                return page.within_bbox((left, top, right, bottom)).extract_text()\n","            else:\n","                return \"\"\n","\n","        # Extract text from both columns\n","        left_text = extract_column_text(page, left_column)\n","        right_text = extract_column_text(page, right_column)\n","\n","        # Combine and add to content\n","        content_plumber += left_text + \"\\n\" + right_text + \"\\n\"  # Adjust separator if needed\n","\n","    # print(content_plumber)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":470,"status":"ok","timestamp":1724140803729,"user":{"displayName":"Jefferson Castro Reque","userId":"00769565088665449256"},"user_tz":300},"id":"DyN5YMFbyE-7","outputId":"f387761b-5cfd-481f-e031-c4e68b0bd967"},"outputs":[],"source":["content_plumber = re.sub(r'\\s*\\b\\d{2,3}\\b\\s*', '\\n', content_plumber)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"executionInfo":{"elapsed":671,"status":"ok","timestamp":1724145407396,"user":{"displayName":"Jefferson Castro Reque","userId":"00769565088665449256"},"user_tz":300},"id":"KZaoVe_FzOm7","outputId":"a865167a-eba8-4593-ba42-212491dcb63c"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1519, 4)\n"]}],"source":["# Función para dividir la traducción en español e inglés\n","def split_translation(category, translation):\n","    # Eliminar el numero entre paréntesis al final de la categoría\n","    category = re.sub(r'\\(\\d+\\)', '', category)\n","    # Dividir la traducción usando la categoría como referencia\n","    parts = translation.split(f\"{category} \")\n","    spanish_translation = parts[0].strip()\n","    english_translation = parts[1].strip() if len(parts) > 1 else \"\"\n","    return spanish_translation, english_translation\n","\n","# Funcion para poner en Mayuscula la primera letra despues de un punto\n","def capitalize_after_period(text):\n","    # Encuentra todos los puntos y la letra siguiente\n","    def capitalize_match(match):\n","        return match.group(1) + match.group(2).upper()\n","    # Usar una expresión regular para encontrar los puntos seguidos de una letra\n","    capitalized_text = re.sub(r'([.!?]\\s+)(\\w)', capitalize_match, text)\n","    return capitalized_text\n","\n","def extract_entries(text):\n","    # Expresión regular para encontrar las entradas del vocabulario y sus categorías\n","    pattern = r'\\b([a-záéíóúñA-ZÁÉÍÓÚÑ][^\\n]*?)\\s+(pron\\.|v\\.|V\\.|\\(.\\) v\\.|\\(.\\) V\\.|adj\\.|n\\.|N\\.|\\(.\\) n\\.|\\(.\\) N\\.|num\\.|adv\\.|int\\.)'\n","\n","    # Buscar todas las coincidencias en el texto\n","    matches = list(re.finditer(pattern, text))\n","\n","    # Filtrar entradas que tienen puntuación extra al final\n","    filtered_matches = [match for match in matches if not re.search(r'[^\\w\\s]', match.group(1).strip()[-1])]\n","\n","    # Crear un diccionario para almacenar las traducciones asociadas a las entradas\n","    vocabulary_entries = defaultdict(lambda: {'category': [], 'spanish_meaning': [], 'english_meaning': []})\n","    for i, match in enumerate(filtered_matches):\n","        entry, category = match.groups()\n","        entry = entry.strip()  # Limpiar espacios extra\n","        start_pos = match.end()  # Posición justo después de la entrada\n","\n","        # Determinar la posición de la siguiente entrada o el final del texto\n","        if i < len(filtered_matches) - 1:\n","            next_start_pos = filtered_matches[i + 1].start()\n","        else:\n","            next_start_pos = len(text)\n","\n","        # Extraer la traducción desde la posición de la entrada hasta la siguiente entrada o el final del texto\n","        translation = text[start_pos:next_start_pos].strip()\n","        translation = re.sub(r'\\s+', ' ', translation)  # Eliminar espacios adicionales\n","\n","        # Guardar en el diccionario\n","        if entry and category:\n","            spanish_translation, english_translation = split_translation(category, translation)\n","            vocabulary_entries[entry]['category'].append(category)\n","            vocabulary_entries[entry]['spanish_meaning'].append(capitalize_after_period(spanish_translation))\n","            vocabulary_entries[entry]['english_meaning'].append(capitalize_after_period(english_translation))\n","\n","    # Convertir el diccionario en una lista de diccionarios\n","    data = []\n","    for entry, details in vocabulary_entries.items():\n","        data.append({\n","            'verb': entry,\n","            'category': details['category'],\n","            'spanish_meaning': details['spanish_meaning'],\n","            'english_meaning': details['english_meaning']\n","        })\n","\n","    return data\n","\n","\n","data = extract_entries(content_plumber)\n","df = pd.DataFrame(data)\n","\n","# Imprimir el DataFrame\n","print(df.shape)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":293},"executionInfo":{"elapsed":502,"status":"ok","timestamp":1724145413965,"user":{"displayName":"Jefferson Castro Reque","userId":"00769565088665449256"},"user_tz":300},"id":"r7G-0M_pA4Jx","outputId":"08e822be-d906-46c6-fe11-ae917493f7e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["(480, 3)\n"]}],"source":["df_verbs = df.copy()\n","\n","# Filtrar filas donde 'category' contiene elementos que coinciden con el patrón\n","df_verbs = df_verbs[df_verbs['category'].apply(lambda categories: any(re.search(r'^\\(?\\d*\\)?\\s*v\\.$', category) for category in categories))]\n","\n","# Elimnar spanish_meaning y english_meaning si category no cumple con el patrón r'^\\(?\\d*\\)?\\s*v\\.$'\n","def filter_meanings(categories, spanish_meanings, english_meanings):\n","    new_categories = []\n","    new_spanish_meanings = []\n","    new_english_meanings = []\n","    for category, spanish_meaning, english_meaning in zip(categories, spanish_meanings, english_meanings):\n","        if re.search(r'^\\(?\\d*\\)?\\s*v\\.$', category):\n","            new_categories.append(category)\n","            new_spanish_meanings.append(spanish_meaning)\n","            new_english_meanings.append(english_meaning)\n","    return new_categories, new_spanish_meanings, new_english_meanings\n","\n","\n","# Aplicar la función filter_meanings\n","df_verbs[['category', 'spanish_meaning', 'english_meaning']] = df_verbs.apply(lambda row: filter_meanings(row['category'], row['spanish_meaning'], row['english_meaning']), axis=1, result_type='expand')\n","\n","# Eliminar la columna 'category'\n","df_verbs = df_verbs.drop(columns=['category'])\n","\n","print(df_verbs.shape)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(15, 3)\n","(0, 3)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>verb</th>\n","      <th>spanish_meaning</th>\n","      <th>english_meaning</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>231</th>\n","      <td>bohka iki</td>\n","      <td>[cortarse uno mismo el cerquillo. V. To cut one’s own bangs.]</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>298</th>\n","      <td>chihbini</td>\n","      <td>[ventana. N. Window.]</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>378</th>\n","      <td>habashkipakei</td>\n","      <td>[tornarse un poco oscuro. To become a little dark]</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>399</th>\n","      <td>hanka hanka iki</td>\n","      <td>[fatigarse o agitarse debido a un esfuerzo físico muy prolongado. N. To become fatigued or agitated due to a very prolonged physical effort.]</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>547</th>\n","      <td>iskan okoin</td>\n","      <td>[sudar. V. To sweat.]</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>605</th>\n","      <td>kapói</td>\n","      <td>[preparar una sopa espesa, conocida como mazamorra en el castellano regional. N. To prepare a thick soup, known as mazamorra (milky maize pudding) in the regional Castilian.]</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>647</th>\n","      <td>kesai</td>\n","      <td>[mentir. N. To lie.]</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>762</th>\n","      <td>maspoi</td>\n","      <td>[cubrirse con algo, por ejemplo, con una colcha. N. To cover oneself with something, for example, with a quilt.]</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>829</th>\n","      <td>mewin</td>\n","      <td>[mezclar líquidos. N. To mix liquids.]</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>970</th>\n","      <td>nokoi</td>\n","      <td>[llegar a un lugar. N. To arrive at a place.]</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>1071</th>\n","      <td>pata aki</td>\n","      <td>[hacer bulla. N. To make a commotion.]</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>1094</th>\n","      <td>pen iki</td>\n","      <td>[hacer ruido un árbol al caer. V, for a tree to make noise upon falling.]</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>1131</th>\n","      <td>defecar</td>\n","      <td>[to defecate.]</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>1389</th>\n","      <td>toshia</td>\n","      <td>[reventarse algo, como un huevo o una llanta. N. To burst something, as an egg or a tire.]</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>1413</th>\n","      <td>tsobe</td>\n","      <td>[palabra interrogativa de persona: con quién. N. Interrogative word of person: with who.]</td>\n","      <td>[]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 verb  \\\n","231         bohka iki   \n","298          chihbini   \n","378     habashkipakei   \n","399   hanka hanka iki   \n","547       iskan okoin   \n","605             kapói   \n","647             kesai   \n","762            maspoi   \n","829             mewin   \n","970             nokoi   \n","1071         pata aki   \n","1094          pen iki   \n","1131          defecar   \n","1389           toshia   \n","1413            tsobe   \n","\n","                                                                                                                                                                     spanish_meaning  \\\n","231                                                                                                                    [cortarse uno mismo el cerquillo. V. To cut one’s own bangs.]   \n","298                                                                                                                                                            [ventana. N. Window.]   \n","378                                                                                                                               [tornarse un poco oscuro. To become a little dark]   \n","399                                    [fatigarse o agitarse debido a un esfuerzo físico muy prolongado. N. To become fatigued or agitated due to a very prolonged physical effort.]   \n","547                                                                                                                                                            [sudar. V. To sweat.]   \n","605   [preparar una sopa espesa, conocida como mazamorra en el castellano regional. N. To prepare a thick soup, known as mazamorra (milky maize pudding) in the regional Castilian.]   \n","647                                                                                                                                                             [mentir. N. To lie.]   \n","762                                                                 [cubrirse con algo, por ejemplo, con una colcha. N. To cover oneself with something, for example, with a quilt.]   \n","829                                                                                                                                           [mezclar líquidos. N. To mix liquids.]   \n","970                                                                                                                                    [llegar a un lugar. N. To arrive at a place.]   \n","1071                                                                                                                                          [hacer bulla. N. To make a commotion.]   \n","1094                                                                                                       [hacer ruido un árbol al caer. V, for a tree to make noise upon falling.]   \n","1131                                                                                                                                                                  [to defecate.]   \n","1389                                                                                      [reventarse algo, como un huevo o una llanta. N. To burst something, as an egg or a tire.]   \n","1413                                                                                       [palabra interrogativa de persona: con quién. N. Interrogative word of person: with who.]   \n","\n","     english_meaning  \n","231               []  \n","298               []  \n","378               []  \n","399               []  \n","547               []  \n","605               []  \n","647               []  \n","762               []  \n","829               []  \n","970               []  \n","1071              []  \n","1094              []  \n","1131              []  \n","1389              []  \n","1413              []  "]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Filtrar filas con english_meaning vacío\n","df_with_empty_english_meaning = df_verbs[df_verbs['english_meaning'].apply(lambda meanings: all(meaning == '' for meaning in meanings))]\n","print(df_with_empty_english_meaning.shape)\n","\n","# Filtrar filas con spanish_meaning vacío\n","df_with_empty_spanish_meaning = df_verbs[df_verbs['spanish_meaning'].apply(lambda meanings: all(meaning == '' for meaning in meanings))]\n","print(df_with_empty_spanish_meaning.shape)\n","\n","# Concatenar los DataFrames con significados vacíos\n","df_with_empties = pd.concat([df_with_empty_english_meaning, df_with_empty_spanish_meaning])\n","\n","df_with_empties"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(478, 3)\n"]}],"source":["# Eliminar las filas que no tienen verbos válidos\n","df_verbs = df_verbs[~df_verbs['verb'].isin(['chihbini', 'tsobe'])]\n","\n","# Crear una máscara para seleccionar solo las filas donde 'english_meaning' está vacío y 'spanish_meaning' contiene ' N. ', ' V. ' o ' V, '\n","mask = df_verbs.apply(lambda row: all(meaning == '' for meaning in row['english_meaning']) and any(any(substring in meaning for substring in [' N. ', ' V. ', ' V, ']) for meaning in row['spanish_meaning']), axis=1)\n","\n","# Mostrar las filas seleccionadas\n","df_verbs.loc[mask, ['verb', 'spanish_meaning', 'english_meaning']]\n","\n","# Dividir 'spanish_meaning' en dos columnas 'spanish_meaning' y 'english_meaning' solo para las filas seleccionadas\n","split_columns = df_verbs.loc[mask, 'spanish_meaning'].apply(lambda meanings: [re.split(r' N\\. | V\\. | V, ', meaning) for meaning in meanings])\n","\n","# Asignar las columnas divididas de vuelta a las filas seleccionadas en el DataFrame original\n","df_verbs.loc[mask, 'spanish_meaning'] = split_columns.apply(lambda x: [item[0] for item in x])\n","df_verbs.loc[mask, 'english_meaning'] = split_columns.apply(lambda x: [item[1] if len(item) > 1 else '' for item in x])\n","print(df_verbs.shape)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>verb</th>\n","      <th>spanish_meaning</th>\n","      <th>english_meaning</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>378</th>\n","      <td>habashkipakei</td>\n","      <td>[tornarse un poco oscuro. To become a little dark]</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>1131</th>\n","      <td>defecar</td>\n","      <td>[to defecate.]</td>\n","      <td>[]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["               verb                                     spanish_meaning  \\\n","378   habashkipakei  [tornarse un poco oscuro. To become a little dark]   \n","1131        defecar                                      [to defecate.]   \n","\n","     english_meaning  \n","378               []  \n","1131              []  "]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Filtrar filas con english_meaning o spanish_meaning vacío\n","df_with_empty_english_meaning = df_verbs[df_verbs['english_meaning'].apply(lambda meanings: all(meaning == '' for meaning in meanings))]\n","df_with_empty_spanish_meaning = df_verbs[df_verbs['spanish_meaning'].apply(lambda meanings: all(meaning == '' for meaning in meanings))]\n","df_with_empties = pd.concat([df_with_empty_english_meaning, df_with_empty_spanish_meaning])\n","\n","df_with_empties"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["# Eliminar las filas con verb = 'habashkipakei' o 'defecar'\n","verbs_to_remove = ['habashkipakei', 'defecar', 'poi', 'toai', \"tewe tewe ik\"]\n","\n","df_verbs = df_verbs[~df_verbs['verb'].isin(verbs_to_remove)]\n","\n","# Add new entries\n","new_entries = [\n","    {\n","        \"verb\": \"poi\",\n","        \"spanish_meaning\": [\"defecar.\"],\n","        \"english_meaning\": [\"to defecate.\"]\n","    },\n","    {\n","        \"verb\": \"habashkipakei\",\n","        \"spanish_meaning\": [\"tornarse un poco oscuro.\"],\n","        \"english_meaning\": [\"To become a little dark.\"]\n","    },\n","    {\n","        \"verb\": \"toai\",\n","        \"spanish_meaning\": [\n","            \"cargar (a un niño o a un animal); en el castellano regional se dice amarcar.\",\n","            \"cernir.\",\n","            \"estar embarazada una mujer.\"\n","        ],\n","        \"english_meaning\": [\n","            \"To carry (a child or an animal); in regional Castilian it is called amarcar.\",\n","            \"to sift.\",\n","            \"for a woman to become pregnant. Toa aki\"\n","        ]\n","    },\n","    {\n","        \"verb\": \"tewe tewe iki\",\n","        \"spanish_meaning\": [\n","            \"excitarse, despertarse el deseo sexual en una persona.\"\n","        ],\n","        \"english_meaning\": [\n","            \"to become aroused, to wake up the sexual desire in a person.\"\n","        ],\n","    },\n","]\n","\n","new_entries_df = pd.DataFrame(new_entries)\n","df_verbs = pd.concat([df_verbs, new_entries_df], ignore_index=True)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# Modificar las oraciones que contienen una palabra junto a un número entre paréntesis al final del punto en 'spanish_meaning'\n","df_verbs['english_meaning'] = df_verbs['english_meaning'].apply(lambda meanings: [re.sub(r'\\.\\s\\w+(\\s\\w+)?+(\\s\\w+)?\\s*(\\(\\d+\\))?$', '', meaning) for meaning in meanings])\n","\n","# Modify some entries\n","df_verbs.loc[df_verbs['verb'] == 'chibain', 'english_meaning'] = [[\"to chase or to follow the trail of a person or an animal.\"]]\n","df_verbs.loc[df_verbs['verb'] == 'hená matsi', 'english_meaning'] = [[\"to put out (the fire)\"]]\n","df_verbs.loc[df_verbs['verb'] == 'nawin', 'english_meaning'] = [[\"to eat meat accompanied by something like yucca, potato, banana, or rice.\"]]\n","df_verbs.loc[df_verbs['verb'] == 'noroh aki', 'english_meaning'] = [[\"to slurp, producing a noise with the throat.\"]]\n","df_verbs.loc[df_verbs['verb'] == 'pen pen aki', 'english_meaning'] = [[\"for the floor to make noise upon a stick or similar object hitting it, such as those traditionally made for some iskonawa dances.\"]]\n","df_verbs.loc[df_verbs['verb'] == 'rakai', 'english_meaning'] = [[\"to lie on one’s back.\"]]\n","df_verbs.loc[df_verbs['verb'] == 'tahka tahka iki', 'english_meaning'] = [[\"to shake off; for a serpent to jingle.\"]]\n","df_verbs.loc[df_verbs['verb'] == 'ten ten iki', 'english_meaning'] = [[\"for an injury to hurt, producing stabbing pains.\"]]\n","df_verbs.loc[df_verbs['verb'] == 'toponki', 'english_meaning'] = [[\"to count, to enumerate.\"]]\n","df_verbs.loc[df_verbs['verb'] == 'warepakei', 'english_meaning'] = [[\"to rip articles of clothing or paper various times.\"]]"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"u8vQBWWFB4yA"},"outputs":[{"name":"stdout","output_type":"stream","text":["(477, 4)\n"]}],"source":["# Sort DataFrame by 'verb' column\n","df_verbs = df_verbs.sort_values('verb').reset_index(drop=True)\n","\n","# Add refenrece column\n","df_verbs['reference'] = '17375911/ZFR5PFEG'\n","\n","print(df_verbs.shape)\n","# Guardar el DataFrame en un archivo CSV\n","df_verbs.to_json('../json/isc_vocabulary_verbs.json', orient='records', force_ascii=False)\n","df.to_json('../json/isc_vocabulary.json', orient='records', force_ascii=False)"]},{"cell_type":"markdown","metadata":{"id":"Uc0XmSyUjHSG"},"source":["# Extraer Ejemplos"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":16968,"status":"ok","timestamp":1725883053866,"user":{"displayName":"Jefferson Castro Reque","userId":"00769565088665449256"},"user_tz":300},"id":"sKM2Z7RIlVPz"},"outputs":[],"source":["# Open the PDF file\n","content_bosquejo_gramatical = open_pdf('../pdf/Bosquejo-gramatical.pdf')\n","content_grammatical_relations_iskonawa = open_pdf('../pdf/2015_grammatical_relations_iskonawa.pdf')\n","content_prefijos_iskonawa = open_pdf('../pdf/2015_prefijos_iskonawa.pdf')\n","content_canciones_iskonawa = open_pdf('../pdf/2019_canciones_iskonawa.pdf')"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11867,"status":"ok","timestamp":1725883065732,"user":{"displayName":"Jefferson Castro Reque","userId":"00769565088665449256"},"user_tz":300},"id":"cKFzBd9vjGys","outputId":"18dda072-d4c2-4254-c173-1dcb0ada515d"},"outputs":[],"source":["def extract_verbs(content):\n","    # Definir el máximo número de caracteres permitido por columna\n","    max_length = 500\n","\n","    # Parte 1: Inicio de la línea y captura del número entre paréntesis\n","    inicio_linea_y_numero = r'^\\s*\\((\\d+)\\)\\s+'\n","    # Parte 2: Captura de la oración en Iskonawa hasta el primer salto de línea\n","    iskonawa_sentence = rf'(.{{1,{max_length}}}?)\\s*\\n'\n","    # Parte 3: Captura de la primera línea de sufijos\n","    primera_linea_sufijos = rf'\\s*(.{{1,{max_length}}}?)\\s*\\n'\n","    # Parte 4: Captura de la segunda línea de significados\n","    segunda_linea_significados = rf'\\s*(.{{1,{max_length}}}?)\\s*\\n'\n","    # Parte 5: Captura del significado en español entre comillas\n","    significado_espanol = rf'\\s*[‘\\']\\s*(.{{1,{max_length}}}?)\\s*[’\\']'\n","\n","    # Combinar todas las partes en el regex final\n","    regex = (\n","        inicio_linea_y_numero +\n","        iskonawa_sentence +\n","        primera_linea_sufijos +\n","        segunda_linea_significados +\n","        significado_espanol\n","    )\n","    # Diccionario para almacenar las entradas numeradas\n","    entradas_dict = {}\n","\n","    # Encontrar todas las coincidencias con sus posiciones\n","    matches = re.finditer(regex, content, re.MULTILINE | re.DOTALL)\n","\n","    # Guardar cada coincidencia en el diccionario junto con la posición inicial\n","    for match in matches:\n","        numero = match.group(1)\n","        iskonawa_sentence = match.group(2).strip()\n","        suffix_sentence = match.group(3).strip()\n","        annotated_sentence = match.group(4).strip()\n","        spanish_sentence = match.group(5).strip()\n","        start_position = match.start(2)  # Posición donde comienza iskonawa_sentence\n","        num_lineas_sufijos = suffix_sentence.count('\\n') + 1\n","        num_lineas_significado = annotated_sentence.count('\\n') + 1\n","        num_lineas_spanish_sentence = spanish_sentence.count('\\n') + 1\n","\n","        # Si annotated_sentence tiene 3 líneas, mover la segunda línea a suffix_sentence\n","        if num_lineas_significado == 3:\n","            annotated_sentence_split = annotated_sentence.split('\\n')\n","            suffix_sentence += ' ' + annotated_sentence_split[1].strip()\n","            annotated_sentence = f'{annotated_sentence_split[0]} {annotated_sentence_split[2]}'\n","            num_lineas_significado = 1  # Ajustar el número de líneas\n","\n","        # En suffix_sentence si hay un espacio seguido de un guion, se debe eliminar el espacio\n","        # Ejemplo: sachavaca flaco Germán -ERG matar -PERF -> sachavaca flaco Germán-ERG matar-PERF\n","        suffix_sentence = re.sub(r'\\s+-', '-', suffix_sentence)\n","        annotated_sentence = re.sub(r'\\s+-', '-', annotated_sentence)\n","\n","\n","        entradas_dict[numero] = {\n","            'index': numero,\n","            'iskonawa_sentence': iskonawa_sentence,\n","            'suffix_sentence': suffix_sentence,\n","            'annotated_sentence': annotated_sentence,\n","            'spanish_sentence': spanish_sentence,\n","            'start_position': start_position,\n","            'num_lineas_sufijos': num_lineas_sufijos,\n","            'num_lineas_significado': num_lineas_significado,\n","            'num_lineas_spanish_sentence': num_lineas_spanish_sentence        \n","        }\n","\n","        # Contar numero de lineas en annotated_sentence\n","    return entradas_dict"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[],"source":["def extract_verbs_v2(content):\n","\n","    # reemplazar „ por ‘ y ‟ por ’\n","    content = content.replace('„', '‘').replace('‟', '’')\n","\n","    # Definir el máximo número de caracteres permitido por columna\n","    max_length = 500\n","\n","    # Parte 1: Inicio de la línea y captura del número entre paréntesis\n","    inicio_linea_y_numero = r'^\\s*\\((\\d+[a-zA-Z]?)\\)\\s+'\n","    # Parte 2: Captura de la oración en Iskonawa hasta el primer salto de línea\n","    primera_linea_sufijos = rf'(.{{1,{max_length}}}?)\\s*\\n'\n","    # Parte 4: Captura de la segunda línea de significados\n","    segunda_linea_significados = rf'\\s*(.{{1,{max_length}}}?)\\s*\\n'\n","    # Parte 5: Captura del significado en español entre comillas\n","    significado_espanol = rf'\\s*[‘\\']\\s*(.{{1,{max_length}}}?)\\s*[’\\']'\n","\n","    # Combinar todas las partes en el regex final\n","    regex = (\n","        inicio_linea_y_numero +\n","        primera_linea_sufijos +\n","        segunda_linea_significados +\n","        significado_espanol\n","    )\n","    # Diccionario para almacenar las entradas numeradas\n","    entradas_dict = {}\n","\n","    # Encontrar todas las coincidencias con sus posiciones\n","    matches = re.finditer(regex, content, re.MULTILINE | re.DOTALL)\n","\n","    # Guardar cada coincidencia en el diccionario junto con la posición inicial\n","    for match in matches:\n","        numero = match.group(1)\n","        iskonawa_sentence = match.group(2).strip()\n","        suffix_sentence = match.group(2).strip()\n","        annotated_sentence = match.group(3).strip()\n","        spanish_sentence = match.group(4).strip()\n","        start_position = match.start(1)  # Posición donde comienza iskonawa_sentence\n","        num_lineas_sufijos = suffix_sentence.count('\\n') + 1\n","        num_lineas_significado = annotated_sentence.count('\\n') + 1\n","        num_lineas_spanish_sentence = spanish_sentence.count('\\n') + 1\n","\n","        # Si annotated_sentence tiene 3 líneas, mover la segunda línea a suffix_sentence\n","        if num_lineas_significado == 3:\n","            annotated_sentence_split = annotated_sentence.split('\\n')\n","            suffix_sentence += ' ' + annotated_sentence_split[1].strip()\n","            annotated_sentence = f'{annotated_sentence_split[0]} {annotated_sentence_split[2]}'\n","            num_lineas_significado = 1  # Ajustar el número de líneas\n","\n","        # En suffix_sentence si hay un espacio seguido de un guion, se debe eliminar el espacio\n","        # Ejemplo: sachavaca flaco Germán -ERG matar -PERF -> sachavaca flaco Germán-ERG matar-PERF\n","        suffix_sentence = re.sub(r'\\s+-', '-', suffix_sentence)\n","        annotated_sentence = re.sub(r'\\s+-', '-', annotated_sentence)\n","\n","        # iskonawa_sentence sera suffix_sentence sin los guiones\n","        iskonawa_sentence = re.sub(r'-', '', suffix_sentence)\n","        iskonawa_sentence = re.sub(r'–', '', iskonawa_sentence)\n","        iskonawa_sentence = re.sub(r'ø', '', iskonawa_sentence)\n","        spanish_sentence = re.sub(r'Y o', 'Yo', spanish_sentence)\n","\n","\n","        entradas_dict[numero] = {\n","            'index': numero,\n","            'iskonawa_sentence': iskonawa_sentence,\n","            'suffix_sentence': suffix_sentence,\n","            'annotated_sentence': annotated_sentence,\n","            'spanish_sentence': spanish_sentence,\n","            'start_position': start_position,\n","            'num_lineas_sufijos': num_lineas_sufijos,\n","            'num_lineas_significado': num_lineas_significado,\n","            'num_lineas_spanish_sentence': num_lineas_spanish_sentence        \n","        }\n","\n","        # Contar numero de lineas en annotated_sentence\n","    return entradas_dict\n","\n","\n"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["244\n"]}],"source":["entries_bosquejo_gramatical = extract_verbs(content_bosquejo_gramatical)\n","df_bgi = pd.DataFrame.from_dict(entries_bosquejo_gramatical, orient='index')\n","df_bgi['reference'] = '17375911/7XWQXF6G'\n","df_bgi = df_bgi.drop_duplicates(subset=['iskonawa_sentence'])\n","print(len(df_bgi))"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["17\n"]}],"source":["entries_grammatical_relations_iskonawa = extract_verbs_v2(content_grammatical_relations_iskonawa)\n","df_gri = pd.DataFrame.from_dict(entries_grammatical_relations_iskonawa, orient='index')\n","df_gri['reference'] = '17375911/GSDUJ7ZH'\n","df_gri = df_gri.drop_duplicates(subset=['iskonawa_sentence'])\n","print(len(df_gri))"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["105\n"]}],"source":["entries_prefijos_iskonawa = extract_verbs_v2(content_prefijos_iskonawa)\n","df_pi = pd.DataFrame.from_dict(entries_prefijos_iskonawa, orient='index')\n","df_pi['reference'] = '17375911/PJJRIJT4'\n","df_pi = df_pi.drop_duplicates(subset=['iskonawa_sentence'])\n","print(len(df_pi))"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["23\n"]}],"source":["def clean_text(text):\n","    # Reemplazar caracteres no deseados\n","    lines = text.split('\\n')\n","    lines = [line for line in lines if \"Ah eh eh\" not in line]\n","    text = '\\n'.join(lines)\n","    text = re.sub(r'\\((\\d+[a-zA-Z]?)\\)', r'\\n(\\1) ', text)\n","    text = re.sub(r'අඈർ', 'LOC ', text)\n","    text = re.sub(r'අඈർ', 'LOC ', text)\n","    text = re.sub(r'ർඈඉ', 'COP ', text)\n","    text = re.sub(r'ඌංൻ', 'SIB', text)\n","    text = re.sub(r'ඉඈඌඌ', 'POSS', text)\n","    text = re.sub(r'ർൺඎඌ-ංඉൿඏ', 'CAUS-IPFV', text)\n","    text = re.sub(r'ർൺඎඌ-ංආඉ', 'CAUS-IMP', text)\n","    text = re.sub(r' ංඉൿඏ', 'IPFV', text)\n","    text = re.sub(r'ඉൿඏ', 'PFV ', text)\n","    text = re.sub(r'ൾඏංൽ', 'EVID', text)\n","    text = re.sub(r'ඉඈඌඉ', 'POSP', text)\n","    text = re.sub(r'ඉඋඈඉ', 'PROP', text)\n","    text = re.sub(r'ඌ඀', 'SG', text)\n","    text = re.sub(r'ඈ', 'O ', text)\n","    text = re.sub(r'ඌ', 'S', text)\n","    text = re.sub(r'’', '’', text)\n","    return text\n","\n","entries_canciones_iskonawa = clean_text(content_canciones_iskonawa)\n","entries_canciones_iskonawa = extract_verbs(entries_canciones_iskonawa)\n","df_ci = pd.DataFrame.from_dict(entries_canciones_iskonawa, orient='index')\n","df_ci['reference'] = '17375911/JYUAVIM5'\n","df_ci = df_ci.drop_duplicates(subset=['iskonawa_sentence'])\n","print(len(df_ci))\n"]},{"cell_type":"code","execution_count":22,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(389, 10)\n"]}],"source":["df_temp = pd.concat([df_bgi, df_gri, df_pi, df_ci])\n","print(df_temp.shape)"]},{"cell_type":"markdown","metadata":{"id":"kYXy_n8flwHz"},"source":["### Clean data"]},{"cell_type":"code","execution_count":23,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_293666/3360959130.py:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_errors['index'] = df_errors.index\n"]}],"source":["df_errors = df_temp[(df_temp['num_lineas_significado'] > 1) | (df_temp['num_lineas_sufijos'] > 1) | (df_temp['num_lineas_spanish_sentence'] > 1)]\n","df_errors['index'] = df_errors.index"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# Print df_errors nicely\n","def print_dataframe(df):\n","    for row in df.itertuples():\n","        print(f\"Reference: {row.reference}\")\n","        print(f\"Index: {row.index}\")\n","        print(f\"Oración Iskonawa: {row.iskonawa_sentence}\")\n","        print(f\"Oración Sufijos: {row.suffix_sentence}\")\n","        print(f\"Oración Significado: {row.annotated_sentence}\")\n","        print(f\"Oración Español: {row.spanish_sentence}\")\n","        print(\"-\" * 50)\n","\n","# print_dataframe(df_errors)"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(342, 6)\n"]}],"source":["# rows to drop\n","rows_to_drop = ['48', '77', '78', '79', '87', '89', '106', '114', '165', '215', '249', '285','287']\n","new_df_bgi = df_bgi[~df_bgi.index.isin(rows_to_drop)]\n","\n","\n","rows_to_drop = ['1a']\n","new_df_gri = df_gri[~df_gri.index.isin(rows_to_drop)]\n","rows_to_drop = ['1a', '1b', '1c', '1d', '2a', '2b', '2c', '2d',\n","                '8a', '8b',\n","                '9a', '9b', '9c',\n","                '10a', '10b', '10c', '11a', '11b', '11c', '11d', '11e',\n","                '12a', '12b', '12c', '13', '14a', '14b', '14c',\n","                '16a', '16b', '16c', '20',\n","                '21a', '21b', '21c', '21d', '22', '23', '24', '28a', '28b', '28c', '32a']\n","new_df_pi = df_pi[~df_pi.index.isin(rows_to_drop)]\n","\n","\n","new_df = pd.concat([new_df_bgi, new_df_gri, new_df_pi, df_ci])\n","\n","# Eliminar las columnas 'start_position', 'num_lineas_sufijos' y 'num_lineas_significado'\n","new_df = new_df.drop(columns=['start_position', 'num_lineas_sufijos', 'num_lineas_significado', 'num_lineas_spanish_sentence'])\n","\n","#Eliminar cambio de linea en spanish_sentence\n","new_df['spanish_sentence'] = new_df['spanish_sentence'].str.replace('\\n', ' ')\n","\n","print(new_df.shape)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[],"source":["data_file_path = 'manually_corrected_data.json'\n","data_df = pd.read_json(data_file_path)\n","\n","\n","for index, item in data_df.iterrows():\n","    new_df.loc[index, 'iskonawa_sentence'] = item[\"iskonawa_sentence\"]\n","    new_df.loc[index, 'index'] = item[\"index\"]\n","    new_df.loc[index, 'suffix_sentence'] = item[\"suffix_sentence\"]\n","    new_df.loc[index, 'annotated_sentence'] = item[\"annotated_sentence\"]\n","    new_df.loc[index, 'spanish_sentence'] = item[\"spanish_sentence\"]\n","    new_df.loc[index, 'reference'] = item[\"reference\"]\n"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["# Guardar el DataFrame en un archivo JSON\n","new_df.to_json('../json/isc_raw_extraction.json', orient='records', force_ascii=False)"]},{"cell_type":"markdown","metadata":{},"source":["## Correciones manuales\n","\n","Se realizan correciones para solucionar:\n","- La mayoria de casos en los `iskonawa_sentence` y `suffix_sentence` no coinciden.\n","- Traducciones"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["new_df = pd.read_json('../json/isc_raw_extraction.json')"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Remove non Iskonawa sentences:\n","Before (355, 6)\n","After (353, 6)\n"]}],"source":["spanish_sentence_replacements = [\n","    (r\"\\(.*?\\)\", \"\"),\n","    (r\"Germán se está volviendo flaco\", \"Germán está adelgazando\"),\n","    (r\"lavar se \", \"lavarse \"),\n","    (r\"Yo voy duermo dentro del monte\", \"Yo duermo dentro del monte\"),\n","    (r\" o rinar\", \" orinar\"),\n","    (r\" s iempre\", \"  siempre\"),\n","    (r\"Germá n\", \"Germán\"),\n","\n","]\n","\n","\n","iskonawa_sentence_replacements = [\n","    (r\"\\(.*?\\)\", \"\"),\n","    (r\"\\. \", \" \"),\n","    (r\" \\.\", \"\"),\n","    (r\"\\.\", \"\"),\n","    (r\"\\*\", \"\"),\n","    (r\"\\(.*?\\)\", \"\"),\n","    (r\" o aka \", \" oaka \"),\n","    (r\" piko ta \", \" piko ta \"),\n","    (r\"ta nkara\", \"tankara\"),\n","    (r\"tankar a\", \"tankara\"),\n","    (r\"meach ia\", \"meachia\"),\n","    (r\"Meritani n\", \"Meritanin\"),\n","    (r\"Germa nin\", \"Germanin\"),\n","    (r\"piko ta\", \"pikota\"),\n","    (r\"aho n\", \"ahon\"),\n","    (r\"cho kikerana\", \"chokikerana\"),\n","    (r\"Po caripani\", \"Pocaripani\"),\n","    (r\"kah eranabi\", \"kaheranabi\"),\n","    (r\"abe rana\", \"aberana\"),\n","    (r\"mer ita\", \"merita\"),\n","    (r\"c arorinabetan\", \"carorinabetan\"),\n","    (r\"b akenpa\", \"bakenpa\"),\n","    (r\"rateheko i\", \"ratehekoi\"),\n","    (r\"Emenki ri\", \"Emenkiri\"),\n","    (r\"o nin\", \"onin\"),\n","    (r\"bo ankatsi\", \"boankatsi\"),\n","    (r\"yo ishika\", \"yoishika\"),\n","    (r\"rateheko ni\", \"ratehekoni\"),\n","    (r\"cho ki\", \"choki\"),\n","    (r\"o hakerana\", \"ohakerana\"),\n","    (r\"o hakaina\", \" ohakaina\"),\n","    (r\"ch oro\", \"choro\"),\n","    (r\"choro npakea\", \"choronpakea\"),\n","    (r\"ayayo hma\", \"ayayohma\"),\n","    (r\"ayo hma\", \"ayohma\"),\n","    (r\"bo anrana\", \"boanrana\"),\n","    (r\"mahkako ana\", \"mahkakoana\"),\n","    (r\"Migueln in\", \"Miguelnin\"),\n","    (r\"pipak ea\", \"pipakea\"),\n","    (r\"o hmai\", \"ohmai\"),\n","    (r\"inso n\", \"inson\"),\n","    (r\"p otapakea\", \"potapakea\"),\n","    (r\"hecho makoin\", \"hechomakoin\"),\n","    (r\"bikeho n\", \"bikehon\"),\n","    (r\"carorinabetanho n\", \"carorinabetanhon\"),\n","    (r\"ho ntinti\", \"hontinti\"),\n","    (r\"German k a mena iki\", \"German ka mena iki\"),\n","    (r\"nono nkesa\", \"nononkesa\"),\n","    (r\"Troh ishonko\", \"Trohishonko\"),\n","    (r\"piboko na\", \"pibokona\"),\n","    (r\"maba in\", \"mabain\"),\n","    (r\"Edelvi nanin\", \"Edelvinanin\"),\n","    (r\"chipa si\", \"chipasi\"),\n","    (r\"rohko37\", \"rohko\"),\n","    (r\"E ah \", \"Eah \"),\n","    (r\"peshte i\", \"peshtei\"),\n","    (r\"oin makin\", \"oinmakin\"),\n","]\n","\n","suffix_sentence_replacements = [\n","    (r\"\\(.*?\\)\", \"\"),\n","    (r\"\\[\", \"\"),\n","    (r\"\\]\", \"\"),\n","    (r\"Anton io\", \"Antonio\"),\n","    (r\"Germa n-nin\", \"German-nin\"),\n","    (r\"- \", \"-\"),\n","    (r\" -\", \"-\"),\n","    (r\"nono n-kesa\", \"nonon-kesa\"),\n","    (r\"Troh isho-nko\", \"Trohisho-nko\"),\n","    (r\"pi-boko n-a\", \"pi-bokon-a\"),\n","    (r\"Edelvi na-nin\", \"Edelvina-nin\"),\n","    (r\"tsi- \", \"tsi-\"),\n","    (r\"chi-pa si\", \"chi-pasi\"),\n","    (r\"rohko37\", \"rohko\"),\n","    (r\"E ah \", \"Eah \"),\n","]\n","\n","annotated_sentence_replacements = [\n","    (r\"- \", \"-\"),\n","    (r\" -\", \"-\"),\n","    (r\"1.SG.S Chachi.Bai-LOC ir-IMPF\", \"1.SG.S Chachi Bai-LOC ir-IMPF\"),\n","    (r\"1.SG.ABS Chachi.Bai-LOC ir-IMPF\", \"1.SG.ABS Chachi Bai-LOC ir-IMPF\"),\n","    (r\"1SG.S río adentro bañars e-viniendo-PERF\", \"1SG.S río adentro bañarse-viniendo-PERF\"),\n","    (r\"eah Troh isho-nko ka-i\", \"eah Trohisho-nko ka-i\"),\n","    (r\"golpear .con.palo-MAL-VEN.TRAN-PERF\", \"golpear.con.palo-MAL-VEN.TRAN-PERF\"),\n","    (r\"Jeshuc o\", \"Jeshuco\"),\n","    (r\"golpear .con.palo\", \"golpear.con.palo\"),\n","    (r\"agarrar-PERF \\(<NOMLZ\\?\\)-PLU\", \"agarrar-PERF(<NOMLZ?)-PLU\"),\n","    (r\"Marlonbran do\", \"Marlonbrando\"),\n","    (r\"persona .ABS\", \"persona.ABS\"),\n","    (r\" \\d \", \" \"),\n","    (r\"P ERF\", \"PERF\"),\n","    (r\"2sg. ACUS\", \"2sg.ACUS\"),\n","    (r\"pintar. con\", \"pintar.con\"),\n","    (r\"lavar se\", \"lavarse\"),\n","    (r\"bañars e\", \"bañarse\"),\n","    (r\"v\\s+agina\", \"vagina\"),\n","    (r\"pec\\s+ho\", \"pecho\"),\n","    (r\"1sg .A\", \"1sg.A\"),\n","    (r\"espalda/omóplato/ ala, pluma/caparazón\", \"ala\"),\n","    (r\"llev ar-PL-PERF\", \"llevar-PL-PERF\"),\n","]\n","\n","def clean_sentence(sentence, replacements):\n","    for pattern, replacement in replacements:\n","        sentence = re.sub(pattern, replacement, sentence)\n","    return sentence\n","\n","\n","new_df['spanish_sentence'] = new_df['spanish_sentence'].apply(lambda x: clean_sentence(x, spanish_sentence_replacements))\n","new_df['iskonawa_sentence'] = new_df['iskonawa_sentence'].apply(lambda x: clean_sentence(x, iskonawa_sentence_replacements))\n","new_df['suffix_sentence'] = new_df['suffix_sentence'].apply(lambda x: clean_sentence(x, suffix_sentence_replacements))\n","new_df['annotated_sentence'] = new_df['annotated_sentence'].apply(lambda x: clean_sentence(x, annotated_sentence_replacements))\n","\n","\n","# Todo elimiar De Bosquejo Gramatical las filas con la columna index igual a 96 y 97, pues no son de Iskonawa\n","print(\"Remove non Iskonawa sentences:\")\n","print(\"Before\", new_df.shape)\n","new_df = new_df[~(((new_df['index'] == '159') | (new_df['index'] == '160')) & new_df['reference'].str.contains(\"17375911/7XWQXF6G\"))]\n","print(\"After\", new_df.shape)"]},{"cell_type":"markdown","metadata":{},"source":["En `df_filtered` se listan aquellos casos cuya diferencia entre `iskonawa_sentence` y `suffix_sentence` no es debido a erroes de extracción, sino a diferencias en el propio libro."]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(12, 6)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>iskonawa_sentence</th>\n","      <th>suffix_sentence</th>\n","      <th>annotated_sentence</th>\n","      <th>reference</th>\n","      <th>index</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5</th>\n","      <td>Oni chahu ka ikon iki</td>\n","      <td>oni chahu  ki iki</td>\n","      <td>gente  venado  EVI.2  COP</td>\n","      <td>17375911/7XWQXF6G</td>\n","      <td>44</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Chaho  retea  ena oina</td>\n","      <td>chaho   rete-a   oin-a</td>\n","      <td>venado.ABS  matar-NOMLZ  ver-PERF</td>\n","      <td>17375911/7XWQXF6G</td>\n","      <td>52</td>\n","    </tr>\n","    <tr>\n","      <th>62</th>\n","      <td>Koni ewan ka icha waka iki</td>\n","      <td>koni-n    ewa    ka     icha        waka   mena  iki</td>\n","      <td>anguila-GEN   madre  EVI.1  mucho   río     dentro  COP</td>\n","      <td>17375911/7XWQXF6G</td>\n","      <td>113</td>\n","    </tr>\n","    <tr>\n","      <th>129</th>\n","      <td>Edelvina oinsi</td>\n","      <td>Edelvina  mia  oin-is-i</td>\n","      <td>Edelvina  2SG.ACU  ver-PROG-IMPF</td>\n","      <td>17375911/7XWQXF6G</td>\n","      <td>189</td>\n","    </tr>\n","    <tr>\n","      <th>220</th>\n","      <td>Oinkin!  ~ Oinwe!</td>\n","      <td>oin-kin   oin-we</td>\n","      <td>ver-IMP  ver-IMP</td>\n","      <td>17375911/7XWQXF6G</td>\n","      <td>286</td>\n","    </tr>\n","    <tr>\n","      <th>261</th>\n","      <td>tsi soi</td>\n","      <td>tsi-soi</td>\n","      <td>trasero-limpiar</td>\n","      <td>17375911/PJJRIJT4</td>\n","      <td>17a</td>\n","    </tr>\n","    <tr>\n","      <th>262</th>\n","      <td>tsi siki</td>\n","      <td>tsi-siki</td>\n","      <td>trasero-mirar.REFL</td>\n","      <td>17375911/PJJRIJT4</td>\n","      <td>17b</td>\n","    </tr>\n","    <tr>\n","      <th>264</th>\n","      <td>tsi tsomi</td>\n","      <td>tsi-tsomi</td>\n","      <td>trasero-pellizcar</td>\n","      <td>17375911/PJJRIJT4</td>\n","      <td>17d</td>\n","    </tr>\n","    <tr>\n","      <th>300</th>\n","      <td>pa choki</td>\n","      <td>pa-choki</td>\n","      <td>oreja-lavarse</td>\n","      <td>17375911/PJJRIJT4</td>\n","      <td>30d</td>\n","    </tr>\n","    <tr>\n","      <th>309</th>\n","      <td>Isabel     bepara hako</td>\n","      <td>Isabel     be-para-hako</td>\n","      <td>Isabel     frente-aplanado-DIM</td>\n","      <td>17375911/PJJRIJT4</td>\n","      <td>33a</td>\n","    </tr>\n","    <tr>\n","      <th>310</th>\n","      <td>Isabel  bepara koin</td>\n","      <td>Isabel  be-para-koin</td>\n","      <td>Isabel   frente-aplanado-LGT</td>\n","      <td>17375911/PJJRIJT4</td>\n","      <td>33b</td>\n","    </tr>\n","    <tr>\n","      <th>311</th>\n","      <td>hete  hete si</td>\n","      <td>hete  hete-s-i</td>\n","      <td>oler   oler-SUF-IMPF</td>\n","      <td>17375911/PJJRIJT4</td>\n","      <td>34a</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["              iskonawa_sentence  \\\n","5        Oni chahu ka ikon iki    \n","11     Chaho  retea  ena oina     \n","62   Koni ewan ka icha waka iki   \n","129              Edelvina oinsi   \n","220           Oinkin!  ~ Oinwe!   \n","261                     tsi soi   \n","262                    tsi siki   \n","264                   tsi tsomi   \n","300                    pa choki   \n","309      Isabel     bepara hako   \n","310         Isabel  bepara koin   \n","311               hete  hete si   \n","\n","                                          suffix_sentence  \\\n","5                                       oni chahu  ki iki   \n","11                                 chaho   rete-a   oin-a   \n","62   koni-n    ewa    ka     icha        waka   mena  iki   \n","129                               Edelvina  mia  oin-is-i   \n","220                                      oin-kin   oin-we   \n","261                                               tsi-soi   \n","262                                              tsi-siki   \n","264                                             tsi-tsomi   \n","300                                              pa-choki   \n","309                               Isabel     be-para-hako   \n","310                                  Isabel  be-para-koin   \n","311                                        hete  hete-s-i   \n","\n","                                          annotated_sentence  \\\n","5                                  gente  venado  EVI.2  COP   \n","11                         venado.ABS  matar-NOMLZ  ver-PERF   \n","62   anguila-GEN   madre  EVI.1  mucho   río     dentro  COP   \n","129                         Edelvina  2SG.ACU  ver-PROG-IMPF   \n","220                                         ver-IMP  ver-IMP   \n","261                                          trasero-limpiar   \n","262                                       trasero-mirar.REFL   \n","264                                        trasero-pellizcar   \n","300                                            oreja-lavarse   \n","309                           Isabel     frente-aplanado-DIM   \n","310                             Isabel   frente-aplanado-LGT   \n","311                                     oler   oler-SUF-IMPF   \n","\n","             reference index  \n","5    17375911/7XWQXF6G    44  \n","11   17375911/7XWQXF6G    52  \n","62   17375911/7XWQXF6G   113  \n","129  17375911/7XWQXF6G   189  \n","220  17375911/7XWQXF6G   286  \n","261  17375911/PJJRIJT4   17a  \n","262  17375911/PJJRIJT4   17b  \n","264  17375911/PJJRIJT4   17d  \n","300  17375911/PJJRIJT4   30d  \n","309  17375911/PJJRIJT4   33a  \n","310  17375911/PJJRIJT4   33b  \n","311  17375911/PJJRIJT4   34a  "]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["df_filtered = new_df.copy()\n","df_filtered = df_filtered[df_filtered.apply(lambda x: len(x['iskonawa_sentence'].split()) != len(x['suffix_sentence'].split()), axis=1)]\n","\n","print(df_filtered.shape)\n","df_filtered[['iskonawa_sentence', 'suffix_sentence', 'annotated_sentence', 'reference', 'index']]"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["dict_acronyms = {\n","    '17375911/L2ZRAXYS': 'TOIC',\n","    '17375911/ZFR5PFEG': 'VICI',\n","    '17375911/7XWQXF6G': 'BGLI',\n","    '17375911/JYUAVIM5': 'IRCF',\n","    '17375911/PJJRIJT4': 'PPCI',\n","    '17375911/GSDUJ7ZH': 'RGLI',\n","}\n","\n","# Crear una función para mapear los valores de 'reference' a sus acrónimos\n","def map_reference_to_acronym(reference):\n","    return dict_acronyms.get(reference, '')\n","\n","\n","# Aplicar la función a la columna 'reference' y concatenar con el índice\n","new_df['key'] = new_df['reference'].apply(map_reference_to_acronym) + new_df['index'].astype(str)\n"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["# En las columnas 'suffix_sentence' y 'annotated_sentence', los espacios deben ser maximo 1\n","# Eliminar espacios adicionales\n","new_df['iskonawa_sentence'] = new_df['iskonawa_sentence'].str.replace(r'\\s+', ' ', regex=True)\n","new_df['suffix_sentence'] = new_df['suffix_sentence'].str.replace(r'\\s+', ' ', regex=True)\n","new_df['annotated_sentence'] = new_df['annotated_sentence'].str.replace(r'\\s+', ' ', regex=True)\n","new_df['spanish_sentence'] = new_df['spanish_sentence'].str.replace(r'\\s+', ' ', regex=True)\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(353, 7)\n"]}],"source":["print(new_df.shape)\n","new_df.to_json('../json/isc_sentences.json', orient='records', force_ascii=False)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOqcZ1wcVwYx2LpMhF1M6U0","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.6"}},"nbformat":4,"nbformat_minor":0}
